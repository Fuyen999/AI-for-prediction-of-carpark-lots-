{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "american-saint",
   "metadata": {},
   "source": [
    "# MINI PROJECT\n",
    "## Project Name: SG CARPARKS\n",
    "\n",
    "### By: Group 5\n",
    "- Wang Sunmeng\n",
    "- Wong Ting Wen Adelina\n",
    "- Yew Fu Yen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-diameter",
   "metadata": {},
   "source": [
    "### Problem Definition:\n",
    "> How many available carpark lots are there near a certain location at a certain time ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-chancellor",
   "metadata": {},
   "source": [
    "#### First, we import all the libraries we need for this project:\n",
    "\n",
    "1. pandas, json, numpy: for data representation\n",
    "2. datetime: to handle time data\n",
    "3. os: for file manipulation on local device\n",
    "4. gspread, df2gspread, googleapiclient.discovery: to integrate data collection with google sheets and google drive\n",
    "5. ServiceAccountCredentials: for google api credentials\n",
    "6. time: to implement sleep function\n",
    "7. geopy: for geopatial search and distance calculation function\n",
    "8. geocoder: to get user's IP location\n",
    "9. seaborn, matplotlib.pyplot: for data visualization\n",
    "10. sklearn modules: for machine learning\n",
    "\n",
    "#### (please download these libraries on your local device before running the code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "collaborative-campbell",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "#For API request and file/data manipulation\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "#GSheets Stuff\n",
    "import gspread\n",
    "from df2gspread import df2gspread as d2g\n",
    "from df2gspread import gspread2df as g2d\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from googleapiclient import discovery\n",
    "\n",
    "#For Sleep\n",
    "import time\n",
    "\n",
    "#For Geocoding\n",
    "from geopy.geocoders import Nominatim\n",
    "# import geocoder\n",
    "\n",
    "#For Graphing\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "sb.set()\n",
    "\n",
    "#Linreg\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-express",
   "metadata": {},
   "source": [
    "### Data collection:\n",
    "1. download_from_google_sheets()\n",
    "    - a function that downloads all of our collected data from google drive to the local device.\n",
    "    - The data will be stored in .csv format and in a folder called \"Data\"\n",
    "    - The \"Data\" folder should be created in the currect directory before running the code\n",
    "    - After the data is downloaded to the local device, it would take a much shorter time for pandas to read the data and convert them into dataframes \n",
    "\n",
    "### Short list of what we did in the code:\n",
    "1. Check if the data for a certain date is already present on the local device\n",
    "2. If not, fetch the data from the spreadsheet of that certain date\n",
    "3. Add headers to the data\n",
    "4. Concatenate different spreadsheets depending on how the data is collected\n",
    "4. Download the data in .csv format into the \"./Data\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "explicit-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download data stored in google sheets to local device.\n",
    "#all spreadsheet keys stored at https://docs.google.com/spreadsheets/d/1bms8J3Hiv_F3Mycsr14gwVZiJi1-ngLj0fNhdRkIAwQ/edit#gid=0\n",
    "#downloaded data has header\n",
    "def download_from_google_sheets():\n",
    "    # Credential stuff\n",
    "    scope = [\"https://spreadsheets.google.com/feeds\",\n",
    "            \"https://www.googleapis.com/auth/drive\"]\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(\"./creds.json\", scope)\n",
    "    gc = gspread.authorize(credentials)\n",
    "\n",
    "    ID_of_spreadsheet_keys = \"1bms8J3Hiv_F3Mycsr14gwVZiJi1-ngLj0fNhdRkIAwQ\"\n",
    "    ID_of_one_set_data = \"1J6c50eUbTeOLbMQ88dTpB5lV_fEg2l9Ke2rdhVGW9Mw\"\n",
    "    worksheet_name = 'Sheet1'\n",
    "\n",
    "    #open sheet that stores all spreadsheet keys\n",
    "    sheet = gc.open_by_key(ID_of_spreadsheet_keys)\n",
    "    worksheet = sheet.worksheet(worksheet_name)\n",
    "    spreadsheet_keys = {}\n",
    "    dates_raw = worksheet.col_values(1)\n",
    "    IDs = worksheet.col_values(2)\n",
    "    for i in range(len(IDs)):\n",
    "        spreadsheet_keys[(int(dates_raw[i].split(\"/\")[0]), int(dates_raw[i].split(\"/\")[1]))] = IDs[i]\n",
    "\n",
    "    #check existing data in ./Data/\n",
    "    existing_data = os.listdir(\"./Data/\")\n",
    "    existing_dates = [(int(i.split(\"_\")[0]), int(i.split(\"_\")[1])) for i in existing_data]\n",
    "    for date in spreadsheet_keys.keys():\n",
    "        if date not in existing_dates:\n",
    "            #open sheet, select worksheet\n",
    "            sheet = gc.open_by_key(spreadsheet_keys[date])\n",
    "            worksheet = sheet.worksheet(worksheet_name)\n",
    "\n",
    "            #download values into a dataframe\n",
    "            df = pd.DataFrame(worksheet.get_all_values())\n",
    "\n",
    "            #for data before 26/3/2021: only have 4 columns.\n",
    "            #assume that \"C\" lotType carpark always have more available slots than other types of carpark. Remove duplicated data with same carpark ID.\n",
    "            if (len(df.columns) == 4):\n",
    "                df.columns = [\"Time\", \"Date\", \"CarParkID\",\"AvailableLots\"]\n",
    "                df = df.sort_values(by=[\"CarParkID\", \"AvailableLots\"])\n",
    "                df = df.drop_duplicates(subset=[\"CarParkID\", \"Time\"], keep=\"last\")\n",
    "                \n",
    "                #open sheet that stores one set of data (without time, date and availableLots)\n",
    "                sheet = gc.open_by_key(ID_of_one_set_data)\n",
    "                worksheet = sheet.worksheet(worksheet_name)\n",
    "                one_set_data = pd.DataFrame(worksheet.get_all_values())\n",
    "                one_set_data.columns = [\"CarParkID\",\"Area\",\"Development\", \"Location\", \"LotType\", \"Agency\"]\n",
    "                one_set_data = pd.DataFrame(one_set_data[one_set_data[\"LotType\"] == \"C\"])\n",
    "                \n",
    "                df = df.merge(one_set_data, on=\"CarParkID\")\n",
    "                sheet = gc.open_by_key(spreadsheet_keys[date])\n",
    "                \n",
    "            #for data since 26/3/2021\n",
    "            #leave only \"C\" LotType carpark\n",
    "            elif (len(df.columns) == 9):\n",
    "                df.columns = [\"Time\", \"Date\", \"CarParkID\", \"Area\",\"Development\", \"Location\", \"AvailableLots\", \"LotType\", \"Agency\"]\n",
    "                df = pd.DataFrame(df[df[\"LotType\"] == \"C\"])\n",
    "\n",
    "            #export as csv\n",
    "            filename = os.path.join(\"./Data/\", ((sheet.title+\".csv\").replace('/','_')))\n",
    "            df.to_csv(filename, index=False)\n",
    "            \n",
    "            print((sheet.title+\".csv\").replace('/','_'), \"downloaded\")\n",
    "        time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-power",
   "metadata": {},
   "source": [
    "### Data Curation and Preparation:\n",
    "> To solve our problem, we have to clean our data so that it contains only several carparks that are within a certain range from a desired location.\n",
    "\n",
    "#### To achieve this, we have to:\n",
    "   1. get the user's input location\n",
    "   2. create a new dataframe that contains only the data of nearby carparks\n",
    "\n",
    "### Functions:\n",
    "#### 1. Get Search Location:\n",
    "   - uses geopy library\n",
    "   - gives several possible locations through a keyword search\n",
    "   - ask user to choose a location, then returns the coordinates or that location \n",
    "    \n",
    "    \n",
    "#### 2. Get User Location:\n",
    "   - uses geocoder library\n",
    "   - returns the coordinates of the user's device's IP location\n",
    "    \n",
    "    \n",
    "#### 3. Carparks Nearby:\n",
    "   - using coordinates returned from get_search/user_location, calculate the bbox coordinates of a certain radius\n",
    "   - strip out data from the input dataframe so that it contains only the data from relevant carparks within the bbox range\n",
    "   - add columns to store latitude and longitude of each carpark in the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "academic-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns coordinates of desired search result chosen by user\n",
    "def get_search_location():\n",
    "    #This uses HERE api but seems not as good as geopy\n",
    "    \"\"\"import requests\n",
    "    import pandas as pd\n",
    "    \n",
    "    URL = \"https://geocode.search.hereapi.com/v1/geocode\"\n",
    "    #input(\"Enter the location here: \")\n",
    "    api_key = \"VhUOdWKYgNfrPSPdaYcBarB9OLFvT-rLAqW4wBc-Wy0\" # Acquired from developer.here.com\n",
    "    PARAMS = {'apikey':api_key,'q':str(search_term)} \n",
    "\n",
    "    # sending get request and saving the response as response object \n",
    "    response = requests.get(url = URL, params = PARAMS)\n",
    "    location = pd.DataFrame(response.json())\n",
    "    location = pd.DataFrame(location[\"items\"].apply(pd.Series))\n",
    "    \n",
    "    return location\"\"\"\n",
    "    \n",
    "    #This uses Nominatim api through geopy\n",
    "    geolocator = Nominatim(user_agent=\"user-636@project-306014.iam.gserviceaccount.com\")\n",
    "    search = geolocator.geocode\n",
    "    \n",
    "    while True:\n",
    "        search_term = input(\"Please enter a location: \")\n",
    "\n",
    "        location = search(search_term + \", Singapore\", exactly_one = False) #exactly_one = False to return more than one result\n",
    "        location = pd.DataFrame(location)\n",
    "        if location.empty == True:\n",
    "            print(\"No relevant search result.\\n\")\n",
    "            continue\n",
    "        location = pd.concat([location.iloc[:,:2], location[1].apply(pd.Series)], axis=1) #split lat and lng\n",
    "        location.columns = [\"address\",\"coordinates\", \"lat\", \"lng\"]\n",
    "\n",
    "        print(\"\\nDo you mean?\")\n",
    "        for i in range(len(location[\"address\"])):\n",
    "            print(str(i+1) + \". \" + location[\"address\"][i])\n",
    "        print(str(len(location[\"address\"])+1) + \". Try again\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                choice = int(input(\"Your choice: \"))\n",
    "                if choice > len(location[\"address\"]) + 1:\n",
    "                    print(\"Invalid response!\")\n",
    "                else:\n",
    "                    break\n",
    "            except:\n",
    "                print(\"Invalid response!\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        if choice != len(location[\"address\"]) + 1:\n",
    "            break\n",
    "        \n",
    "    return location[\"coordinates\"][choice-1]\n",
    "\n",
    "#returns coordinates of user's ip location\n",
    "def get_user_location():\n",
    "    g=geocoder.ip(\"me\")\n",
    "    location = pd.json_normalize(g.json)\n",
    "    return (location[\"lat\"], location[\"lng\"])\n",
    "\n",
    "#return a dataset of all carpark within search radius. Include data for every time\n",
    "def carparks_nearby(df, coordinates, radius):\n",
    "    df[\"Location\"] = df[\"Location\"].str.split(\" \", n=1, expand=False)\n",
    "    df = pd.concat([df[[\"Time\", \"Date\", \"Day\", \"CarParkID\", \"Area\",\"Development\", \"Location\", \"AvailableLots\", \"LotType\", \"Agency\"]], (pd.DataFrame(df[\"Location\"].to_list(), columns=['lat', 'lng']))], axis=1)\n",
    "    df['lat'] = pd.to_numeric(df['lat'], errors='coerce')\n",
    "    df['lng'] = pd.to_numeric(df['lng'], errors='coerce')\n",
    "\n",
    "    radius_in_degree = radius/111.2\n",
    "    df = pd.DataFrame(df[abs(df[\"lat\"]-coordinates[0]) < radius_in_degree])\n",
    "    df = pd.DataFrame(df[abs(df[\"lng\"]-coordinates[1]) < radius_in_degree])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-heater",
   "metadata": {},
   "source": [
    "### Data Curation and Preparation:\n",
    "> Prepare data for further analysis\n",
    "\n",
    "#### Get Data:\n",
    "- Further cleans the data\n",
    "- Depending on whether the input date is a weekday or weekend, pick only data that corresponds with the date\n",
    "- Add in a column in the dataframe to indicate which day of a week the data is from\n",
    "- Strip out data from public holidays (public holiday's data is considered as an outlier)\n",
    "- Convert data to suitable data types (Time -> timedelta in seconds, Date-> datetime, Lots -> int64, others -> str)\n",
    "- Output the cleaned dataframe for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "refined-brazil",
   "metadata": {},
   "outputs": [],
   "source": [
    "#coordinates is a tuple of (latitude, longitude) representing the coordinates of the location requested\n",
    "#predDate is a string in the format dd/mm/yyyy\n",
    "#byDay is a bool, if True: if predDate is on Monday, display only Monday data; if False: display all weekday/weekend data\n",
    "def getData(coordinates, predDate, byDay): \n",
    "    df = pd.DataFrame(columns=[\"Time\", \"Date\", \"Day\", \"CarParkID\",\"Area\",\"Development\", \"Location\", \"AvailableLots\", \"LotType\", \"Agency\", \"lat\", \"lng\"])\n",
    "    predDay = datetime.strptime(predDate, '%d/%m/%Y').weekday()\n",
    "    \n",
    "    #single out public holiday\n",
    "    public_holiday = [\"2/4/2021\"]\n",
    "    \n",
    "    for filename in os.listdir(\"./Data/\"):\n",
    "        if filename.endswith(\".csv\"): \n",
    "            date = \"/\".join(filename.split(\"_\")[:3])\n",
    "            day = datetime.strptime(date, '%d/%m/%Y').weekday()\n",
    "            \n",
    "            if byDay:\n",
    "                condition = (day == predDay) and (date not in public_holiday)\n",
    "            else:\n",
    "                condition = (day < 5 and predDay < 5 and date not in public_holiday) or ((day >= 5 and predDay >= 5) or date in public_holiday) and (date != \"2/4/2021\")\n",
    "            \n",
    "            if condition: \n",
    "                path = os.path.join(\"./Data/\", filename)\n",
    "                df_full = pd.read_csv(path, dtype=\"string\")\n",
    "                df_full[\"Day\"] = str(day)\n",
    "                df = df.append(carparks_nearby(df_full, coordinates, 1))\n",
    "                #print(date)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    df['Time'] = pd.to_timedelta(df['Time'])\n",
    "    df['Time'] = df['Time'].dt.total_seconds().div(60).astype(int)\n",
    "    df['AvailableLots'] = pd.to_numeric(df['AvailableLots'])\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format=\"%d/%m/%Y\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-carpet",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "> In order to predict the number of carparks in the future, we have to approximate and find out the relationship between the number of available carpark lots with time. Since both of these data can be represented in numeric form, we figure that regression would be a good way to find out their relationship.\n",
    "\n",
    "> However, from exploratory data analysis, we see that the number of carpark lots do not vary linearly with time. We noticed that the number of available lots varies mostly in a curve-like manner. Most of them have a parabolic curve pattern, while some exhibit other patterns like w-shaped curves and other irregular curves. With that in mind, we conclude that the linear regression method that we have learnt in class would not be a good way for us to solve the problem.\n",
    "\n",
    "> To account with the many types of curve patterns found in our data, we decide to explore another regression method, namely polynoial regression. In polynomial regression, instead of fitting the data into a best fit linear equation, it fits the data into a specified nth degree polynomial equation. The regression line of higher degree polynomials will exhibit different curve patterns with multiple turning points, hence we think that polynomial regression would be a good method for us to solve our problem.\n",
    "\n",
    "> To find out which degree of polynomial fits our data the best, we tried to fit our data with iteratively increasing degree of polynomial, then find out the r^2 value of the model for each degree. Then, we take the model that has the highest r^2 value, which means that the model at that degree of polynomial fits our data the best (least error). The model will then be used to predict the number of carpark lots for a given time.\n",
    "\n",
    "\n",
    "### Functions:\n",
    "#### 1. Polynomial Transform:\n",
    "   - transform the input data into a matrix form that represents nth degree polynomial\n",
    "   - returns the data matrix\n",
    "   \n",
    "#### 2. Fit Model:\n",
    "   - fit the data matrix that represents some nth degree polynomial into the regression model\n",
    "   - returns fitted model\n",
    "\n",
    "#### 3. Make Prediction:\n",
    "   - loops through each carparks nearby\n",
    "   - interatively increase the degree of polynomial and fit the data for each degree\n",
    "   - calculate r^2 value of each model and select the best model (highest r^2)\n",
    "   - use the model selected to predict the number of available lots for each carpark\n",
    "   - every carpark will have a different model so that the model best fits the carpark's data\n",
    "   - print out the number of available carparks and other relevant informations (eg: carpark location, carpark rates, degree of polynomial, r^2 values)\n",
    "   - visualize the results by plotting out the data by scatterplot, regression line by lineplot, and predicted results by barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "structured-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df is a dataframe containing all data of all carparks to be predicted\n",
    "#predhr is an int of hour in 24h format\n",
    "def makePrediction(df, predhr):\n",
    "    print(\"It is predicted that there are:\")\n",
    "    \n",
    "    CarParkID = df[\"CarParkID\"].unique()\n",
    "    f, axes = plt.subplots(len(CarParkID), 2, figsize=(30,12*len(CarParkID)))\n",
    "    legend_map = {'0': 'Monday',\n",
    "                  '1': 'Tuesday',\n",
    "                  '2': 'Wednesday',\n",
    "                  '3': 'Thursday',\n",
    "                  '4': 'Friday'}\n",
    "\n",
    "    counter = 0\n",
    "    for ID in CarParkID:\n",
    "        counter += 1\n",
    "        lotinfo = df[df[\"CarParkID\"] == ID]\n",
    "        lotinfo = lotinfo.sort_values(by=['Date'])\n",
    "        lotinfo = lotinfo.reset_index(drop=True)\n",
    "        \n",
    "        dates = lotinfo[\"Date\"].unique()\n",
    "        test_data_portion = len(dates) // 4\n",
    "\n",
    "        train_data = pd.DataFrame()\n",
    "        test_data = pd.DataFrame()\n",
    "\n",
    "        for i in dates[:-test_data_portion]:\n",
    "            train_data = train_data.append(lotinfo[lotinfo[\"Date\"] == i])\n",
    "        for i in dates[-test_data_portion:]:\n",
    "            test_data = test_data.append(lotinfo[lotinfo[\"Date\"] ==i])\n",
    "\n",
    "        train_x = pd.DataFrame(train_data['Time'])\n",
    "        train_y = pd.DataFrame(train_data['AvailableLots'])\n",
    "        test_x = pd.DataFrame(test_data['Time'])\n",
    "        test_y = pd.DataFrame(test_data['AvailableLots'])\n",
    "\n",
    "        max_degree = 10\n",
    "        best_degree = 1\n",
    "        r2_train = 0\n",
    "        for degree in range(1, max_degree+1):\n",
    "            model = fitModel(degree, train_x, train_y)\n",
    "            train_y_poly_pred = model.predict(polynomialTransform(degree, train_x))\n",
    "\n",
    "            r2 = r2_score(train_y, train_y_poly_pred)\n",
    "            if r2 > r2_train:\n",
    "                r2_train = r2\n",
    "                best_degree = degree\n",
    "        \n",
    "        model = fitModel(best_degree, train_x, train_y)\n",
    "        train_y_poly_pred = model.predict(polynomialTransform(best_degree, train_x))\n",
    "        #axes[counter-1, 0].scatter(train_data[\"Time\"]/60, train_data[\"AvailableLots\"])\n",
    "        sb.scatterplot(x=train_data[\"Time\"]/60, y=train_data[\"AvailableLots\"], hue=train_data['Day'].map(legend_map), ax=axes[counter-1, 0])\n",
    "        sb.lineplot(x=train_data[\"Time\"]/60, y=pd.DataFrame(train_y_poly_pred)[0], ax=axes[counter-1, 0], linewidth = 5)\n",
    "        \n",
    "        #Calculate test r2 score\n",
    "        test_y_poly_pred = model.predict(polynomialTransform(best_degree, test_x))\n",
    "        r2_test = r2_score(test_y, test_y_poly_pred)\n",
    "        #sb.scatterplot(x=test_data[\"Time\"]/60, y=test_data[\"AvailableLots\"], hue=test_data['Day'].map(legend_map), ax=axes[counter-1, 1])\n",
    "        #sb.lineplot(x=test_data[\"Time\"]/60, y=pd.DataFrame(test_y_poly_pred)[0], ax=axes[counter-1, 1], linewidth = 5)\n",
    "        \n",
    "        #barplot\n",
    "        rows=[]\n",
    "        for a in range(6, 25):\n",
    "            predtime = [[a*60]]\n",
    "            pred = pd.DataFrame(predtime, columns = ['Time'])\n",
    "            slots = round(model.predict(polynomialTransform(best_degree, pred))[0][0])\n",
    "            if slots < 0:\n",
    "                slots = 0\n",
    "            rows.append([a,slots])\n",
    "        carparkslots=pd.DataFrame(rows, columns = ['Time','Predicted carpark slots'])\n",
    "        carparkslots.plot.bar(x=\"Time\", y=\"Predicted carpark slots\", rot=0, ax=axes[counter-1, 1])\n",
    "        \n",
    "        #predict\n",
    "        predtime = [[predhr*60]]\n",
    "        pred = pd.DataFrame(predtime, columns = ['Time'])\n",
    "        slots = round(model.predict(polynomialTransform(best_degree, pred))[0][0])\n",
    "        if slots < 0:\n",
    "            slots = 0\n",
    "        \n",
    "        development = lotinfo[\"Development\"][0]\n",
    "        \n",
    "\n",
    "        rates = getRates(lotinfo[\"Agency\"][0], ID, \"16/4/2021\")\n",
    "        \n",
    "        print(str(counter)+ \". \" + str(slots) + \" carpark slots in \" + development +\" at \"+ str(predhr) +\" 00 HRS. Rates: \" + rates)\n",
    "        print(\"Best degree:\", best_degree)\n",
    "        print(\"Degree of fitness:\", r2_train)\n",
    "        print(\"Degree of fitness to test data:\", r2_test)\n",
    "        print()\n",
    "        \n",
    "def fitModel(degree, x, y):\n",
    "    x_poly = polynomialTransform(degree, x)\n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(x_poly, y)\n",
    "    return linreg\n",
    "\n",
    "def polynomialTransform(degree, x):\n",
    "    poly_reg = PolynomialFeatures(degree=degree)\n",
    "    x_poly = poly_reg.fit_transform(x)\n",
    "    return x_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-insulin",
   "metadata": {},
   "source": [
    "#### Get Rates:\n",
    "   - utilises other static datasets to obtain information about the each carparks' rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "chicken-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRates(agency, ID, predDate):\n",
    "    try:\n",
    "        if agency == \"HDB\":\n",
    "            within_central_area = [\"ACB\", \"BBB\", \"BRB1\", \"CY\", \"DUXM\", \"HLM\", \"KAB\", \"KAM\", \"KAS\", \"PRM\", \"SLS\", \"SR1\", \"SR2\", \"TPM\", \"UCS\", \"WCB\"]\n",
    "            if (ID in within_central_area) and (int(datetime.strftime(predhr, '%H')) > 17):\n",
    "                rates = \"$1.20 per half an hr\"\n",
    "            else:\n",
    "                rates = \"$0.60 per half an hr\"\n",
    "        elif agency == \"LTA\":\n",
    "            LTAdf = pd.read_csv(\"./CarParkRates.csv\")\n",
    "            if(datetime.strptime(predDate, '%d/%m/%Y').weekday() < 6):\n",
    "                rates = LTAdf.loc[LTAdf['CarParkID']==int(ID)][\"WeekDays_Rate_1\"].values[0]\n",
    "            else:\n",
    "                rates = LTAdf.loc[LTAdf['CarParkID']==int(ID)][\"Saturday_Rate\"].values[0]\n",
    "            # URA\n",
    "        else:\n",
    "            URAdf = pd.read_csv(\"./URA.csv\")\n",
    "            if (datetime.strptime(predDate, '%d/%m/%Y').weekday() < 6):\n",
    "                rates = str(URAdf.loc[URAdf['ppCode']==ID][\"weekdayRate\"].values[0])  + \" per half an hr\"\n",
    "            else:\n",
    "                rates = str(URAdf.loc[URAdf['ppCode']==ID][\"sunPHRate\"].values[0])  + \" per half an hr\"\n",
    "    except:\n",
    "        rates = \"unknown\"\n",
    "    return rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-devices",
   "metadata": {},
   "source": [
    "#### Data Handling:\n",
    "   - This is our main program that compiles everything that we have done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "future-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datahandling():\n",
    "    mainpage = '''Options:\n",
    "    1. Upload today's data\n",
    "    2. Update to latest data\n",
    "    3. Search'''\n",
    "    print(mainpage)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            action = int(input())\n",
    "            if action < 1 or action > 3:\n",
    "                print(\"Invalid input!\")\n",
    "            else:\n",
    "                break\n",
    "        except:\n",
    "            print(\"Invalid input!\")\n",
    "            \n",
    "    if action == 1:\n",
    "        upload_to_sheets()\n",
    "        \n",
    "    elif action == 2:\n",
    "        download_from_google_sheets()\n",
    "        \n",
    "    elif action == 3:\n",
    "        coordinates = get_search_location()\n",
    "        date = input(\"Enter the date in d/m/yyyy format: \")\n",
    "        while True:\n",
    "            try:\n",
    "                hour = int(input(\"Enter the hour in 24h format (0 ~ 23): \"))\n",
    "                if hour < 0 or hour > 23:\n",
    "                    print(\"Invalid input!\")\n",
    "                else:\n",
    "                    break\n",
    "            except:\n",
    "                print(\"Invalid input!\")\n",
    "        df = getData(coordinates, date, False)\n",
    "        makePrediction(df, hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-timing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
