{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "optical-living",
   "metadata": {},
   "source": [
    "\n",
    "# MINI PROJECT\n",
    "## Project Name: SG CARPARKS\n",
    "\n",
    "### By: Group 5\n",
    "- Wang Sunmeng\n",
    "- Wong Ting Wen Adelina\n",
    "- Yew Fu Yen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-tiffany",
   "metadata": {},
   "source": [
    "## Main Dataset Used:\n",
    "> LTA DataMall -- Carpark Availability API\n",
    "\n",
    "Data available from the API:\n",
    "1. CarParkID: A unique code for carpark\n",
    "2. Area: Area of development/building:\n",
    "3. Development: Major landmark or address where carpark is located\n",
    "4. Location: Latitude and Longitude, map coordinates of the carpark\n",
    "5. AvailableLots: Number of lots available at point of data retrieval\n",
    "6. LotType: Type of lots (cars, heavy vehicles, motorcycles)\n",
    "7. Agency: Carpark agency (HDB, LTA, URA)\n",
    "_____________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-globe",
   "metadata": {},
   "source": [
    "#### First, we import all the libraries we need for this part of the project:\n",
    "\n",
    "1. pandas, json: for data representation\n",
    "2. datetime: to handle time data\n",
    "3. gspread, df2gspread, googleapiclient.discovery: to integrate data collection with google sheets and google drive\n",
    "4. ServiceAccountCredentials: for google API credentials\n",
    "5. time: to implement sleep function\n",
    "6. os: for file manipulation on local device\n",
    "\n",
    "#### (please download these libraries on your local device before running the code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "otherwise-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "#For API request and file/data manipulation\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "#GSheets Stuff\n",
    "import gspread\n",
    "from df2gspread import df2gspread as d2g\n",
    "from df2gspread import gspread2df as g2d\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from googleapiclient import discovery\n",
    "\n",
    "#For Sleep\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-broadcast",
   "metadata": {},
   "source": [
    "## Problem Definition:\n",
    "\n",
    "The dataset provided by the carpark availability API is a dynamic dataset, which means that the values of the data collected can change depending on what point in time the API is called. From the LTA DataMall API's user guide, we learned that the carpark availability API is updated at a frequency of 1 update per minute. This complicates the data extraction process as we cannot just simply download the dataset like any other static datasets. Instead, we have to consider the time when the data is extracted as well.\n",
    "\n",
    "#### Since the number of available carpark lots changes constantly with time, we started to question if:\n",
    "1. the number of available lots had any correlation with time. \n",
    "2. the trend of change in available lots had any correlation with the carpark location.\n",
    "\n",
    "#### Thus, we formulated our problem as:\n",
    "> How many available carpark lots are there near a certain location at a certain time ?\n",
    "\n",
    "The problem dwells with both the relationship between time and available lots, as well as location and available lots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-justice",
   "metadata": {},
   "source": [
    "_______________________________________________\n",
    "## Data Extraction:\n",
    "\n",
    "Considering the nature of our problem, we identified several data that we need to extract from the carpark availability API:\n",
    "1. A way to label each carpark -- carpark ID as provided in the dataset\n",
    "2. Number of available lots of each carpark -- provided in the dataset\n",
    "3. Location data of each carpark -- coordinates of the carpark as provided in the dataset\n",
    "4. Time when each set of data is extracted -- not provided in the dataset (need to be added during data extraction)\n",
    "\n",
    "#### To analyse how the number of available lots changes with time, time data is very important. \n",
    "- In this project, we decided to collect the data with time ranging from 6.00 am in the morning to 12.00 am midnight.\n",
    "- To do this, we created a while loop that detects the system's time so that the script will start automatically at 6.00 am and end automatically at 12.00am.\n",
    "- We also decided to extract data for every 10 minutes to track how the number of available lots changes with time in a day\n",
    "- The data extraction process will continue for several weeks to get multiple data for each point in time for different days in a week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "executive-detective",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Collection will start at 06 00 HR\n",
      "17:03:52\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-eecd8f6198f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%H\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mstart_hour\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%H:%M:%S\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Keyboard Interrupt, not an error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set start and end time here\n",
    "start_hour = \"06\"\n",
    "end_hour = \"00\"\n",
    "print(\"Data Collection will start at \"+start_hour+\" 00 HR\")\n",
    "      \n",
    "# Start automatically at start_hour (please run the code before sleep every night)\n",
    "while (datetime.now().strftime(\"%H\") != start_hour):\n",
    "    print(datetime.now().strftime(\"%H:%M:%S\"), end=\"\\r\")\n",
    "    time.sleep(1)\n",
    "\n",
    "# Keyboard Interrupt, not an error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-click",
   "metadata": {},
   "source": [
    "### Data Storage:\n",
    "- Since we are collecting data dynamically at a 10 minute frequency, we need a data storage system that is easily manageable so that the dataset can be frequently updated during the data extraction process.\n",
    "- We explored several database options at first, such as PostgreSQL, MySQL and MS SQL.\n",
    "- However at last, we decided to use google sheets as our database to store our data. Updating data to google sheets could be easily done using gspread library functions, while google sheets also allow us to easily collaborate and work on the data together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-station",
   "metadata": {},
   "source": [
    "#### 1. To utilise google sheets API in python, we have to first create a google cloud platform account and get our credentials key to authorize for the usage of google's APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "desirable-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credential stuff\n",
    "scope = [\"https://spreadsheets.google.com/feeds\",\n",
    "         \"https://www.googleapis.com/auth/drive\"]\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(\"./creds.json\", scope)\n",
    "gc = gspread.authorize(credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-richards",
   "metadata": {},
   "source": [
    "#### 2. Next, we create a google drive folder to store our data. Here, we uses the google drive API client to open the folder in the python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imposed-survivor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name new google sheets with in this format: <today's date>_carpark_availability\n",
    "wks_name = 'Sheet1'\n",
    "newsheet_name = \"{}/{}/{}_carpark_availability\".format(int(datetime.now().strftime(\"%d\")),\n",
    "                                                       int(datetime.now().strftime(\"%m\")),\n",
    "                                                       int(datetime.now().strftime(\"%Y\")))\n",
    "# Open the destination google drive folder\n",
    "destFolderId = \"15ZD6Jp7ZeWJLOSm2CxmMzSjJ6fqtLfR_\"\n",
    "drive_service = discovery.build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "# Check if a sheet is already created for the day\n",
    "file_list = drive_service.files().list(q=\"name contains '{}'\".format(newsheet_name),\n",
    "                                       spaces='drive',\n",
    "                                       fields='nextPageToken, files(id, name)',\n",
    "                                       pageToken=None).execute()\n",
    "file_list = file_list.get(\"files\", [])\n",
    "\n",
    "# to delete a file (debug purpose)\n",
    "# for file in file_list:\n",
    "#    drive_service.files().delete(fileId=file['id']).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-vaccine",
   "metadata": {},
   "source": [
    "#### 3. Each day, we will be creating a new spreadsheet to store all data collected for the day. After checking that the spreadsheet hasn't been created for the day, we uses gspread to create a new spreadsheet inside the google drive folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "excited-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks for empty row based on values appearing in 1st N columns\n",
    "def next_available_row(sheet, cols_to_sample=2):\n",
    "    try:\n",
    "        cols = sheet.range(1, 1, sheet.row_count, cols_to_sample)\n",
    "        return max([cell.row for cell in cols if cell.value]) + 1\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "if (file_list == []):\n",
    "    # Create new sheet\n",
    "    body = {'name': newsheet_name,\n",
    "            'mimeType': 'application/vnd.google-apps.spreadsheet',\n",
    "            'parents': [destFolderId]}\n",
    "    newsheet = drive_service.files().create(body=body).execute()\n",
    "    spreadsheet_key = newsheet['id']\n",
    "    new_sheet = gc.open_by_key(spreadsheet_key).worksheet(wks_name)\n",
    "    # Delete unused columns to save space (so that we won't reach google sheets' cell limit)\n",
    "    new_sheet.delete_columns(1, 18)\n",
    "\n",
    "    # Update sheet's link to the google sheet that stores all spreadsheet keys (also inside the google drive folder)\n",
    "    spreadsheet_key_sheet = gc.open_by_key(\"1bms8J3Hiv_F3Mycsr14gwVZiJi1-ngLj0fNhdRkIAwQ\").worksheet(wks_name)\n",
    "    row = next_available_row(spreadsheet_key_sheet, 1)\n",
    "    spreadsheet_key_sheet.update(\"A{}\".format(row),\n",
    "                                 \"{}/{}\".format(datetime.now().strftime(\"%d\"), datetime.now().strftime(\"%m\")))\n",
    "    spreadsheet_key_sheet.update(\"B{}\".format(row), spreadsheet_key)\n",
    "    spreadsheet_key_sheet.update(\"F{}\".format(row),\n",
    "                                 \"https://docs.google.com/spreadsheets/d/{}/edit#gid=0\".format(spreadsheet_key))\n",
    "else:\n",
    "    spreadsheet_key = file_list[0]['id']\n",
    "    \n",
    "# old code (manual spreadhsheet creation, not dynamic)\n",
    "# df = g2d.download(key_to_spreadsheet_key, wks_name, col_names=False, row_names=False, credentials=credentials, start_cell='A1')\n",
    "# spreadsheet_key = list(df[1][df[0] == datetime.now().strftime(\"%d/%m\")])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-trouble",
   "metadata": {},
   "source": [
    "#### 4. Now we can start to collect our data. The LTA DataMall APIs only allow 500 data points to be retrieved per API call. However, there are a total of about 2330 data points for all carparks in Singapore. To correctly retrieve the full set of data each time, we need to list out all the URLs for each set of 500 data points and we can retrieve the full set of data by calling the API at these several URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "starting-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LTA API URL (Total data per set = 2338)\n",
    "url_list = []\n",
    "headers = {'AccountKey': 'opoOfz6bTza7BMCTCy8VFA=='}\n",
    "for skip in range(0, 2500, 500):\n",
    "    url_list.append('http://datamall2.mytransport.sg/ltaodataservice/CarParkAvailabilityv2?$skip=' + str(skip))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-murray",
   "metadata": {},
   "source": [
    "#### 5. Now, we create a pandas dataframe to temporarily store the data collected from each API call. We then insert the date and time when the data is collected into the dataframe to complete our set of data. Finally, we upload this set of data to the spreadsheet that we have created before. The process is repeated again after ten minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "automotive-insertion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted data for 19/04/2021 17:04:05\n",
      "Upload succeeded. Sleeping for 10 min.\n",
      "9 minutes 59 seconds elapsed\n",
      "\n",
      "Extracted data for 19/04/2021 17:14:16\n",
      "Upload succeeded. Sleeping for 10 min.\n",
      "1 minutes 22 seconds elapsed\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-616146b2098d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" minutes \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" seconds elapsed\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# End automatically at end_hour\n",
    "while (datetime.now().strftime(\"%H\") != end_hour):\n",
    "    # create a dataframe to store data\n",
    "    df = pd.DataFrame(\n",
    "        columns=['CarParkID', 'Area', 'Development', 'Location', 'AvailableLots', 'LotType', 'Agency'])\n",
    "    \n",
    "    # call the API and append the data into the dataframe\n",
    "    for url in url_list:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        carpark_info = json.loads(response.text)\n",
    "        df = df.append(pd.DataFrame(carpark_info['value']), ignore_index=True)\n",
    "\n",
    "    # df = pd.DataFrame(df[[\"CarParkID\",\"AvailableLots\"]])\n",
    "\n",
    "    # Insert date and time into dataframe\n",
    "    now = datetime.now()\n",
    "    d_string = now.strftime(\"%d/%m/%Y\")\n",
    "    df.insert(0, 'Date', d_string)\n",
    "    t_string = now.strftime(\"%H:%M:%S\")\n",
    "    df.insert(0, 'Time', t_string)\n",
    "    print(\"Extracted data for \" + d_string + \" \" + t_string)\n",
    "\n",
    "    # Upload to google sheets\n",
    "    row = next_available_row(gc.open_by_key(spreadsheet_key).worksheet(wks_name), 1)\n",
    "    d2g.upload(df, spreadsheet_key, wks_name, col_names=False, row_names=False, clean=False,\n",
    "               credentials=credentials, start_cell='A{}'.format(row))\n",
    "    print(\"Upload succeeded. Sleeping for 10 min.\")\n",
    "\n",
    "    # Sleep for 10 minutes\n",
    "    for i in range(1, 600):\n",
    "        print(str(int(i / 60)) + \" minutes \" + str(i % 60) + \" seconds elapsed\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    \n",
    "# Keyboard Interrupt, not error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-fluid",
   "metadata": {},
   "source": [
    "### You can try running the script here !\n",
    "#### Here is our full data extraction function for the project.\n",
    "\n",
    "Due to jupyter's inability to reliably run the script continuously throughout a day, we exported the code into a python file (UploadToSheets.py) and run it directly from anaconda prompt. We will run the script every night before we sleep and it will automatically extract data from the API starting from 6.00am. The script will keep running for the whole day and stop at 12.00am midnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "prostate-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks for empty row based on values appearing in 1st N columns\n",
    "def next_available_row(sheet, cols_to_sample=2):\n",
    "    try:\n",
    "        cols = sheet.range(1, 1, sheet.row_count, cols_to_sample)\n",
    "        return max([cell.row for cell in cols if cell.value]) + 1\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def upload_to_sheets():\n",
    "    # Run this code every night to scrape data, leave it running for the whole day\n",
    "    # Credential stuff\n",
    "    scope = [\"https://spreadsheets.google.com/feeds\",\n",
    "             \"https://www.googleapis.com/auth/drive\"]\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(\"./creds.json\", scope)\n",
    "    gc = gspread.authorize(credentials)\n",
    "\n",
    "    # Set start and end time here\n",
    "    start_hour = \"06\"\n",
    "    end_hour = \"00\"\n",
    "    print(\"Data Collection will start at \"+start_hour+\" 00 HR.\")\n",
    "\n",
    "    # Start automatically at start_hour (please run the code before sleep every night)\n",
    "    while (datetime.now().strftime(\"%H\") != start_hour):\n",
    "        print(datetime.now().strftime(\"%H:%M:%S\"), end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Name new google sheets with in this format: <today's date>_carpark_availability\n",
    "    wks_name = 'Sheet1'\n",
    "    newsheet_name = \"{}/{}/{}_carpark_availability\".format(int(datetime.now().strftime(\"%d\")),\n",
    "                                                           int(datetime.now().strftime(\"%m\")),\n",
    "                                                           int(datetime.now().strftime(\"%Y\")))\n",
    "    destFolderId = \"15ZD6Jp7ZeWJLOSm2CxmMzSjJ6fqtLfR_\"\n",
    "    drive_service = discovery.build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "    # Check if a sheet is already created for the day\n",
    "    file_list = drive_service.files().list(q=\"name contains '{}'\".format(newsheet_name),\n",
    "                                           spaces='drive',\n",
    "                                           fields='nextPageToken, files(id, name)',\n",
    "                                           pageToken=None).execute()\n",
    "    file_list = file_list.get(\"files\", [])\n",
    "\n",
    "    # to delete a file (debug purpose)\n",
    "    # for file in file_list:\n",
    "    #    drive_service.files().delete(fileId=file['id']).execute()\n",
    "\n",
    "    if (file_list == []):\n",
    "        # Create new sheet\n",
    "        body = {'name': newsheet_name,\n",
    "                'mimeType': 'application/vnd.google-apps.spreadsheet',\n",
    "                'parents': [destFolderId]}\n",
    "        newsheet = drive_service.files().create(body=body).execute()\n",
    "        spreadsheet_key = newsheet['id']\n",
    "        new_sheet = gc.open_by_key(spreadsheet_key).worksheet(wks_name)\n",
    "        new_sheet.delete_columns(1, 18)\n",
    "\n",
    "        # Update sheet's link to the google sheet that stores all spreadsheet keys\n",
    "        spreadsheet_key_sheet = gc.open_by_key(\"1bms8J3Hiv_F3Mycsr14gwVZiJi1-ngLj0fNhdRkIAwQ\").worksheet(wks_name)\n",
    "        row = next_available_row(spreadsheet_key_sheet, 1)\n",
    "        spreadsheet_key_sheet.update(\"A{}\".format(row),\n",
    "                                     \"{}/{}\".format(datetime.now().strftime(\"%d\"), datetime.now().strftime(\"%m\")))\n",
    "        spreadsheet_key_sheet.update(\"B{}\".format(row), spreadsheet_key)\n",
    "        spreadsheet_key_sheet.update(\"F{}\".format(row),\n",
    "                                     \"https://docs.google.com/spreadsheets/d/{}/edit#gid=0\".format(spreadsheet_key))\n",
    "    else:\n",
    "        spreadsheet_key = file_list[0]['id']\n",
    "\n",
    "    # old code\n",
    "    # df = g2d.download(key_to_spreadsheet_key, wks_name, col_names=False, row_names=False, credentials=credentials, start_cell='A1')\n",
    "    # spreadsheet_key = list(df[1][df[0] == datetime.now().strftime(\"%d/%m\")])[0]\n",
    "\n",
    "    print(\"This script will grab data from LTA and upload it to \"\n",
    "          \"https://docs.google.com/spreadsheets/d/\" + spreadsheet_key + \"/edit#gid=0 every 10 min. \")\n",
    "    print(\"Press Ctrl + C to stop.\\n\")\n",
    "\n",
    "    # LTA API URL (Total data per set = 2338)\n",
    "    url_list = []\n",
    "    headers = {'AccountKey': 'opoOfz6bTza7BMCTCy8VFA=='}\n",
    "    for skip in range(0, 2500, 500):\n",
    "        url_list.append('http://datamall2.mytransport.sg/ltaodataservice/CarParkAvailabilityv2?$skip=' + str(skip))\n",
    "\n",
    "    # End automatically at end_hour\n",
    "    while (datetime.now().strftime(\"%H\") != end_hour):\n",
    "        df = pd.DataFrame(\n",
    "            columns=['CarParkID', 'Area', 'Development', 'Location', 'AvailableLots', 'LotType', 'Agency'])\n",
    "        for url in url_list:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            carpark_info = json.loads(response.text)\n",
    "            df = df.append(pd.DataFrame(carpark_info['value']), ignore_index=True)\n",
    "\n",
    "        # df = pd.DataFrame(df[[\"CarParkID\",\"AvailableLots\"]])\n",
    "\n",
    "        # Insert date and time into dataframe\n",
    "        now = datetime.now()\n",
    "        d_string = now.strftime(\"%d/%m/%Y\")\n",
    "        df.insert(0, 'Date', d_string)\n",
    "        t_string = now.strftime(\"%H:%M:%S\")\n",
    "        df.insert(0, 'Time', t_string)\n",
    "        print(\"Extracted data for \" + d_string + \" \" + t_string)\n",
    "\n",
    "        # Upload to google sheets\n",
    "        row = next_available_row(gc.open_by_key(spreadsheet_key).worksheet(wks_name), 1)\n",
    "        d2g.upload(df, spreadsheet_key, wks_name, col_names=False, row_names=False, clean=False,\n",
    "                   credentials=credentials, start_cell='A{}'.format(row))\n",
    "        print(\"Upload succeeded. Sleeping for 10 min.\")\n",
    "\n",
    "        # Sleep for 10 minutes\n",
    "        for i in range(1, 600):\n",
    "            print(str(int(i / 60)) + \" minutes \" + str(i % 60) + \" seconds elapsed\", end=\"\\r\")\n",
    "            time.sleep(1)\n",
    "\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_to_sheets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-rotation",
   "metadata": {},
   "source": [
    "### Data collection:\n",
    "#### 1. download_from_google_sheets()\n",
    "   - a function that downloads all of our collected data from google drive to the local device.\n",
    "   - The data will be stored in .csv format and in a folder called \"Data\"\n",
    "   - The \"Data\" folder should be created in the currect directory before running the code\n",
    "   - After the data is downloaded to the local device, it would take a much shorter time for pandas to read the data and convert them into dataframes \n",
    "\n",
    "#### Short list of what we did in the code:\n",
    "1. Check if the data for a certain date is already present on the local device\n",
    "2. If not, fetch the data from the spreadsheet of that certain date\n",
    "3. Add headers to the data\n",
    "4. Concatenate different spreadsheets depending on how the data is collected\n",
    "4. Download the data in .csv format into the \"./Data\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "handmade-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download data stored in google sheets to local device.\n",
    "#all spreadsheet keys stored at https://docs.google.com/spreadsheets/d/1bms8J3Hiv_F3Mycsr14gwVZiJi1-ngLj0fNhdRkIAwQ/edit#gid=0\n",
    "#downloaded data has header\n",
    "def download_from_google_sheets():\n",
    "    # Credential stuff\n",
    "    scope = [\"https://spreadsheets.google.com/feeds\",\n",
    "            \"https://www.googleapis.com/auth/drive\"]\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(\"./creds.json\", scope)\n",
    "    gc = gspread.authorize(credentials)\n",
    "\n",
    "    ID_of_spreadsheet_keys = \"1bms8J3Hiv_F3Mycsr14gwVZiJi1-ngLj0fNhdRkIAwQ\"\n",
    "    ID_of_one_set_data = \"1J6c50eUbTeOLbMQ88dTpB5lV_fEg2l9Ke2rdhVGW9Mw\"\n",
    "    worksheet_name = 'Sheet1'\n",
    "\n",
    "    #open sheet that stores all spreadsheet keys\n",
    "    sheet = gc.open_by_key(ID_of_spreadsheet_keys)\n",
    "    worksheet = sheet.worksheet(worksheet_name)\n",
    "    spreadsheet_keys = {}\n",
    "    dates_raw = worksheet.col_values(1)\n",
    "    IDs = worksheet.col_values(2)\n",
    "    for i in range(len(IDs)):\n",
    "        spreadsheet_keys[(int(dates_raw[i].split(\"/\")[0]), int(dates_raw[i].split(\"/\")[1]))] = IDs[i]\n",
    "\n",
    "    #check existing data in ./Data/\n",
    "    existing_data = os.listdir(\"./Data/\")\n",
    "    existing_dates = [(int(i.split(\"_\")[0]), int(i.split(\"_\")[1])) for i in existing_data]\n",
    "    for date in spreadsheet_keys.keys():\n",
    "        if date not in existing_dates:\n",
    "            #open sheet, select worksheet\n",
    "            sheet = gc.open_by_key(spreadsheet_keys[date])\n",
    "            worksheet = sheet.worksheet(worksheet_name)\n",
    "\n",
    "            #download values into a dataframe\n",
    "            df = pd.DataFrame(worksheet.get_all_values())\n",
    "\n",
    "            #for data before 26/3/2021: only have 4 columns.\n",
    "            #assume that \"C\" lotType carpark always have more available slots than other types of carpark. Remove duplicated data with same carpark ID.\n",
    "            if (len(df.columns) == 4):\n",
    "                df.columns = [\"Time\", \"Date\", \"CarParkID\",\"AvailableLots\"]\n",
    "                df = df.sort_values(by=[\"CarParkID\", \"AvailableLots\"])\n",
    "                df = df.drop_duplicates(subset=[\"CarParkID\", \"Time\"], keep=\"last\")\n",
    "                \n",
    "                #open sheet that stores one set of data (without time, date and availableLots)\n",
    "                sheet = gc.open_by_key(ID_of_one_set_data)\n",
    "                worksheet = sheet.worksheet(worksheet_name)\n",
    "                one_set_data = pd.DataFrame(worksheet.get_all_values())\n",
    "                one_set_data.columns = [\"CarParkID\",\"Area\",\"Development\", \"Location\", \"LotType\", \"Agency\"]\n",
    "                one_set_data = pd.DataFrame(one_set_data[one_set_data[\"LotType\"] == \"C\"])\n",
    "                \n",
    "                df = df.merge(one_set_data, on=\"CarParkID\")\n",
    "                sheet = gc.open_by_key(spreadsheet_keys[date])\n",
    "                \n",
    "            #for data since 26/3/2021\n",
    "            #leave only \"C\" LotType carpark\n",
    "            elif (len(df.columns) == 9):\n",
    "                df.columns = [\"Time\", \"Date\", \"CarParkID\", \"Area\",\"Development\", \"Location\", \"AvailableLots\", \"LotType\", \"Agency\"]\n",
    "                df = pd.DataFrame(df[df[\"LotType\"] == \"C\"])\n",
    "\n",
    "            #export as csv\n",
    "            filename = os.path.join(\"./Data/\", ((sheet.title+\".csv\").replace('/','_')))\n",
    "            df.to_csv(filename, index=False)\n",
    "            \n",
    "            print((sheet.title+\".csv\").replace('/','_'), \"downloaded\")\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dense-publication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19_3_2021_carpark_availability.csv downloaded\n",
      "20_3_2021_carpark_availability.csv downloaded\n",
      "21_3_2021_carpark_availability.csv downloaded\n",
      "22_3_2021_carpark_availability.csv downloaded\n",
      "23_3_2021_carpark_availability.csv downloaded\n",
      "24_3_2021_carpark_availability.csv downloaded\n",
      "25_3_2021_carpark_availability.csv downloaded\n",
      "26_3_2021_carpark_availability.csv downloaded\n",
      "27_3_2021_carpark_availability.csv downloaded\n",
      "28_3_2021_carpark_availability.csv downloaded\n",
      "29_3_2021_carpark_availability.csv downloaded\n",
      "30_3_2021_carpark_availability.csv downloaded\n",
      "31_3_2021_carpark_availability.csv downloaded\n",
      "1_4_2021_carpark_availability.csv downloaded\n",
      "2_4_2021_carpark_availability.csv downloaded\n",
      "4_4_2021_carpark_availability.csv downloaded\n",
      "5_4_2021_carpark_availability.csv downloaded\n",
      "6_4_2021_carpark_availability.csv downloaded\n",
      "7_4_2021_carpark_availability.csv downloaded\n",
      "8_4_2021_carpark_availability.csv downloaded\n",
      "9_4_2021_carpark_availability.csv downloaded\n",
      "10_4_2021_carpark_availability.csv downloaded\n",
      "11_4_2021_carpark_availability.csv downloaded\n",
      "12_4_2021_carpark_availability.csv downloaded\n",
      "13_4_2021_carpark_availability.csv downloaded\n",
      "14_4_2021_carpark_availability.csv downloaded\n",
      "15_4_2021_carpark_availability.csv downloaded\n",
      "16_4_2021_carpark_availability.csv downloaded\n",
      "19_4_2021_carpark_availability.csv downloaded\n"
     ]
    }
   ],
   "source": [
    "download_from_google_sheets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-spending",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
